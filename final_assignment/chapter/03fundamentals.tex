%!TEX root = ../Final_Assignment_SP_ML4IM_2023.tex
\chapter{Fundamentals}
\label{ch:fundamentals}

% kurz
% timo

\section{YOLO}

\subsection{The YOLO algorithm \label{subsec:YOLO_Alg}} 
	{As the human gaze enables us to intuitively recognise, categorise and perceive objects, our brain, in conjunction with our eyes, is able to see quickly and accurately. These abilities allow us to perform complex tasks, such as cycling, which require the simultaneous use of several senses, with little conscious thought. \citep{Redmon2016}. \\
	The computer can be taught to do this with fast and accurate object recognition algorithms. Current systems use classifiers to recognise objects. This is applied at various points in variable scales in the test image to enable an object to be classified \citep{Redmon2016}. \\ 
	
	\begin{figure}[ht]
		\centering
		\includegraphics*[scale = 1, keepaspectratio, trim=2 2 2 2 ]{images/YOLO/YOLO_detection_system.png}
		\caption[The YOLO object recognition system]{The YOLO object recognition system \citep{Redmon2016}}
		\label{YOLO_Objectdetection}
 	\end{figure}
	
	\glqq You Only Look Once\grqq{} (YOLO) considers object recognition as a single regression problem by calculating directly from image pixel to bounding box coordinates and class probabilities. This algorithm analyses an image only once and directly predicts which objects are present where. As a result, the complexity of the structure of YOLO is very low, as shown in Fig. \ref{YOLO_Objectdetection} \citep{Redmon2016}. \\
	The performance for object detection is increased by training YOLO with complete images. This standardised model offers several advantages over traditional object detection systems \citep{Redmon2016}. \\
	The first advantage of YOLO is the increased performance. This is made possible by the fact that object recognition on images is regarded as a regression problem and therefore no complex pipeline slows down the processing of an image.  \citep{Redmon2016}. 
	Secondly, YOLO analyses an image globally with predictions for object detection. This allows YOLO to reduce the error of confusing background and foreground objects by half compared to Fast R-CNN. This is mainly due to the greater context that YOLO gains from the overall image analysis \citep{Redmon2016}. \\
	The third advantage is that YOLO was trained with generalised representations of objects to increase the error tolerance when applying it to new domains and unexpected inputs, due to the possibility of high generalisation \citep{Redmon2016}. \\
	One disadvantage of YOLO is its accuracy. The algorithm has difficulties in localising some objects, especially small ones \citep{Redmon2016}. 

	YOLOv8 from Ultralytics is used in this project. This offers advantages in terms of performance and accuracy of object detection and is explained in more detail in the next chapter. 
	} 

	\subsection{YOLOv8 von Ultralytics}{ \label{subsec:YOLOv8_theoretic}
	
	In January 2023, Ultralytics released YOLOv8, which is based on YOLOv5. This version contains 5 different models (YOLOv8n (nano), YOLOv8s (small), YOLOv8m (medium), YOLOv8l (large), YOLOv8x (extra large)), which were trained with data sets of different sizes \citep{Terven2023}. 	

	The architecture of this implementation can be divided into 3 parts. These are the backbone, the neck and the head \citep{Terven2023}. \\
	The detection of useful features from the input image takes place in the backbone, which is usually implemented as a CNN \citep{Terven2023}. \\
	The neck is used between the backbone and head to aggregate and refine the features that the backbone outputs \citep{Terven2023}. \\
	The last component is the head, which makes predictions based on the features provided by the backbone and neck. A post-processing step, such as non-maximum suppression (NMS), filters out overlapping predictions so that only the most reliable detections are used \citep{Terven2023}.\\
	To improve performance, especially for object recognition of smaller objects, YOLOv8 uses CloU \citep{Zheng2020} and DFL \citep{Li2020} loss functions for bounding boxloss and binary cross entropy for the classificationloss \citep{Terven2023}. 

	This implementation is used in this project as it meets the requirements for precision and performance. 
	}


\section{Computer Vision 2}
		{ \label{subsec:Computer_Vision_2}
		Computer Vision 2 (CV2) is part of the \glqq Open Source Computer Vision\grqq{} (OpenCV) library \citep{opencv_about}. The current version 4.9.0 was released on 28/12/2023 \citep{opencv_release}. \\
		This library contains algorithms for image and video processing. 
		}


\section{PALMA}
	{The High Performance Cluster of the University of Münster, PALMA (\glqq Paralleles Linux System für Münsteraner Anwender\grqq{}) is a computer cluster with more than 3000 processor cores whose computing power is used for highly computing-intensive science and research \cite{PALMA_Uni_Web}. This cluster was put into operation in 2010 and is operated with CentOS 7 \cite{PALMA_Wiki_OS,PALMA_Uni_Web}.}
			

\section{The camera trap}
A camera trap is a device for taking unobtrusive pictures of animals without disturbing them in their natural habitat \cite{Swann2011}. Due to the fast movement and relatively small size of insects, detecting insects on RGB images is challenging \cite{Gebauer_2024_WACV}. However, a \glqq Dynamic Vision Sensor\grqq{} (DVS) can only record the movement of the insects, which results in very low energy consumption as only the movement in the image is recorded \cite{Gebauer_2024_WACV}. \\
The videos used in this project were recorded with a DVS camera, which also records RGB video. For this reason, the results of both camera systems could be compared using the algorithms.

% \begin{itemize}
% 	\item what is a camera trap
% 	\item how does it work
% 	\item the two cameras (rgb and event stream(?))
% \end{itemize}
